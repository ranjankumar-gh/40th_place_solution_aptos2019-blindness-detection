{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an extension of [this](https://www.kaggle.com/kageyama/fork-of-fastai-blindness-detection-resnet34) notebook, which extends upon [this](https://www.kaggle.com/kageyama/fastai-blindness-detection-resnet34) notebook.\n",
    "\n",
    "In this experiment, I try ordinal variables using [this](https://arxiv.org/abs/0704.1028) technique. Basically, I transform the targets to look like multilabel classification, then apply this method for making predictions:\n",
    "\n",
    "> \"...our methods scans output nodes in the order O1, O2,....,OK. It stop when the output of a node is smaller than the predefined threshold T (e.g. 0.5) or no nodes left. The index k of the last node Ok whose output is bigger than T is the predicted category of the data point.\"\n",
    "\n",
    "So basically, I'll apply sigmoid to the model's outputs, then threshold at 0.5 and find the position one before the first zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting efficientnet-pytorch\n",
      "  Downloading https://files.pythonhosted.org/packages/12/f8/35453605c6c471fc406a137a894fb381b05ae9f174b2ca4956592512374e/efficientnet_pytorch-0.4.0.tar.gz\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from efficientnet-pytorch) (1.2.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torch->efficientnet-pytorch) (1.17.0)\n",
      "Building wheels for collected packages: efficientnet-pytorch\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.4.0-cp36-none-any.whl size=11149 sha256=fdb6cb6030edb1b19aa2e090e5046aa8e56c6b7bb23a2a1ccb1edcc1c14f493d\n",
      "  Stored in directory: /tmp/.cache/pip/wheels/27/56/13/5bdaa98ca8bd7d5da65cc741987dd14391b87fa1a09081d17a\n",
      "Successfully built efficientnet-pytorch\n",
      "Installing collected packages: efficientnet-pytorch\n",
      "Successfully installed efficientnet-pytorch-0.4.0\n"
     ]
    }
   ],
   "source": [
    "! pip install efficientnet-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "# Ignore  the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data visualisation and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "\n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import Callback\n",
    "\n",
    "# specifically for manipulating zipped images and getting numpy arrays of pixel values of images.\n",
    "import cv2                  \n",
    "import numpy as np  \n",
    "from tqdm import tqdm\n",
    "import os                   \n",
    "from random import shuffle  \n",
    "from zipfile import ZipFile\n",
    "from PIL import Image\n",
    "from sklearn.utils import shuffle\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "# print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level                                               path\n",
       "0      0  data/old_data/resized_train_cropped/resized_tr...\n",
       "1      0  data/old_data/resized_train_cropped/resized_tr...\n",
       "2      4  data/old_data/resized_train_cropped/resized_tr...\n",
       "3      0  data/old_data/resized_train_cropped/resized_tr...\n",
       "4      0  data/old_data/resized_train_cropped/resized_tr...\n",
       "5      0  data/old_data/resized_train_cropped/resized_tr...\n",
       "6      0  data/old_data/resized_train_cropped/resized_tr...\n",
       "7      0  data/old_data/resized_train_cropped/resized_tr...\n",
       "8      3  data/old_data/resized_train_cropped/resized_tr...\n",
       "9      4  data/old_data/resized_train_cropped/resized_tr..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_image_dir1 = 'data/old_data/'\n",
    "# os.path.join('..', 'input/aptos2019-blindness-detection/')\n",
    "train_dir1 = os.path.join(base_image_dir1,'resized_train_cropped/resized_train_cropped/')\n",
    "df1 = pd.read_csv(os.path.join(base_image_dir1, 'trainLabels_cropped.csv'))\n",
    "df1['path'] = df1['image'].map(lambda x: os.path.join(train_dir1,'{}.jpeg'.format(x)))\n",
    "df1 = df1.drop(columns=['image', 'Unnamed: 0', 'Unnamed: 0.1'])\n",
    "df1 = df1.sample(frac=1, random_state=42).reset_index(drop=True) #shuffle dataframe\n",
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>data/new_data/train_images/90960ddf4d14.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>data/new_data/train_images/4e0656629d02.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>data/new_data/train_images/3b018e8b7303.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>data/new_data/train_images/55eb405ec71e.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>data/new_data/train_images/207dd0487264.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>data/new_data/train_images/1d74c4713e21.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>data/new_data/train_images/eb175669d789.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>data/new_data/train_images/b49b2fac2514.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>data/new_data/train_images/4a44cc840ebe.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>data/new_data/train_images/d6283ded6aea.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level                                         path\n",
       "0      0  data/new_data/train_images/90960ddf4d14.png\n",
       "1      1  data/new_data/train_images/4e0656629d02.png\n",
       "2      3  data/new_data/train_images/3b018e8b7303.png\n",
       "3      4  data/new_data/train_images/55eb405ec71e.png\n",
       "4      0  data/new_data/train_images/207dd0487264.png\n",
       "5      0  data/new_data/train_images/1d74c4713e21.png\n",
       "6      0  data/new_data/train_images/eb175669d789.png\n",
       "7      4  data/new_data/train_images/b49b2fac2514.png\n",
       "8      2  data/new_data/train_images/4a44cc840ebe.png\n",
       "9      2  data/new_data/train_images/d6283ded6aea.png"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_image_dir2 = 'data/new_data/'\n",
    "# os.path.join('..', 'input/aptos2019-blindness-detection/')\n",
    "train_dir2 = os.path.join(base_image_dir2,'train_images/')\n",
    "df2 = pd.read_csv(os.path.join(base_image_dir2, 'train.csv'))\n",
    "df2['path'] = df2['id_code'].map(lambda x: os.path.join(train_dir2,'{}.png'.format(x)))\n",
    "df2 = df2.drop(columns=['id_code'])\n",
    "df2 = df2.sample(frac=1, random_state=42).reset_index(drop=True) #shuffle dataframe\n",
    "df2.columns = ['level', 'path']\n",
    "\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level                                               path\n",
       "0      0  data/old_data/resized_train_cropped/resized_tr...\n",
       "1      0  data/old_data/resized_train_cropped/resized_tr...\n",
       "2      4  data/old_data/resized_train_cropped/resized_tr...\n",
       "3      0  data/old_data/resized_train_cropped/resized_tr...\n",
       "4      0  data/old_data/resized_train_cropped/resized_tr...\n",
       "5      0  data/old_data/resized_train_cropped/resized_tr...\n",
       "6      0  data/old_data/resized_train_cropped/resized_tr...\n",
       "7      0  data/old_data/resized_train_cropped/resized_tr...\n",
       "8      3  data/old_data/resized_train_cropped/resized_tr...\n",
       "9      4  data/old_data/resized_train_cropped/resized_tr..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "# df = df.sample(frac=1).reset_index(drop=True) #shuffle dataframe\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2011a53e9e8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.level.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(diagnosis):\n",
    "    return ','.join([str(i) for i in range(diagnosis + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df.level.apply(get_label)\n",
    "df2['label'] = df.level.apply(get_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level</th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "      <td>0,1,2,3,4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "      <td>0,1,2,3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>data/old_data/resized_train_cropped/resized_tr...</td>\n",
       "      <td>0,1,2,3,4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level                                               path      label\n",
       "0      0  data/old_data/resized_train_cropped/resized_tr...          0\n",
       "1      0  data/old_data/resized_train_cropped/resized_tr...          0\n",
       "2      4  data/old_data/resized_train_cropped/resized_tr...  0,1,2,3,4\n",
       "3      0  data/old_data/resized_train_cropped/resized_tr...          0\n",
       "4      0  data/old_data/resized_train_cropped/resized_tr...          0\n",
       "5      0  data/old_data/resized_train_cropped/resized_tr...          0\n",
       "6      0  data/old_data/resized_train_cropped/resized_tr...          0\n",
       "7      0  data/old_data/resized_train_cropped/resized_tr...          0\n",
       "8      3  data/old_data/resized_train_cropped/resized_tr...    0,1,2,3\n",
       "9      4  data/old_data/resized_train_cropped/resized_tr...  0,1,2,3,4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idx = [i for i in range(len(df1), len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(img,tol=7):        \n",
    "    mask = img>tol\n",
    "    return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "\n",
    "\n",
    "def crop_image1(img,tol=7):\n",
    "    # img is image data\n",
    "    # tol  is tolerance\n",
    "        \n",
    "    mask = img>tol\n",
    "    return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "\n",
    "def crop_image_from_gray(img,tol=7):\n",
    "    if img.ndim ==2:\n",
    "        mask = img>tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img>tol\n",
    "        \n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
    "            return img # return original image\n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "    #         print(img1.shape,img2.shape,img3.shape)\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "    #         print(img.shape)\n",
    "        return img\n",
    "\n",
    "def open_aptos2019_image(fn, convert_mode, after_open)->Image:\n",
    "    SIZE = sz\n",
    "    image = cv2.imread(fn)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = crop_image(image)\n",
    "    image = cv2.resize(image, (SIZE, SIZE))\n",
    "    image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image , (0,0) , SIZE/10) ,-4 ,128)\n",
    "    return Image(pil2tensor(image, np.float32).div_(255))\n",
    "\n",
    "\n",
    "def crop_image_ilovescience(path):\n",
    "    image = cv2.imread(path)\n",
    "    output = image.copy()\n",
    "    output = cv2.cvtColor(output, cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    ret,gray = cv2.threshold(gray,10,255,cv2.THRESH_BINARY)\n",
    "    contours,hierarchy = cv2.findContours(gray,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        print('no contours!')\n",
    "        flag = 0\n",
    "        return image, flag\n",
    "    cnt = max(contours, key=cv2.contourArea)\n",
    "    ((x, y), r) = cv2.minEnclosingCircle(cnt)\n",
    "    x = int(x); y = int(y); r = int(r)\n",
    "    flag = 1\n",
    "    #print(x,y,r)\n",
    "    if r > 100:\n",
    "        return output[0 + (y-r)*int(r<y):-1 + (y+r+1)*int(r<y),0 + (x-r)*int(r<x):-1 + (x+r+1)*int(r<x)], flag\n",
    "    else:\n",
    "        print('none!')\n",
    "        flag = 0\n",
    "        return image,flag\n",
    "    \n",
    "    \n",
    "\n",
    "def load_ben_color(fn, convert_mode, after_open)->Image:\n",
    "    sigmaX=10\n",
    "    IMG_SIZE = sz\n",
    "    image = cv2.imread(fn)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = crop_image_from_gray(image)\n",
    "    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    image=cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , sigmaX) ,-4 ,128)\n",
    "        \n",
    "    return Image(pil2tensor(image, np.float32).div_(255))\n",
    "\n",
    "def load_ben_color_pre(fn, aug=True):\n",
    "    sigmaX=10\n",
    "    IMG_SIZE = sz\n",
    "    try:\n",
    "      image = cv2.imread(fn)\n",
    "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "      image = crop_image_from_gray(image)\n",
    "      if aug==True:\n",
    "          image = apply_album(image, IMG_SIZE)\n",
    "      image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "      image=cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , sigmaX) ,-4 ,128)\n",
    "      return image\n",
    "    except:\n",
    "      print(fn)\n",
    "      pass\n",
    "\n",
    "def open_aptos2019_image_pre(fn):\n",
    "    SIZE = sz\n",
    "    image = cv2.imread(fn)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = crop_image(image)\n",
    "    image = cv2.resize(image, (SIZE, SIZE))\n",
    "    image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image , (0,0) , SIZE/10) ,-4 ,128)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyImageItemList(ImageList):\n",
    "    def open(self, fn:PathOrStr)->Image:\n",
    "#         img = load_ben_color_pre(fn.replace('/./','').replace('//','/'), aug=False)\n",
    "        img, f = crop_image_ilovescience(fn.replace('/./','').replace('//','/'))\n",
    "        # This ndarray image has to be converted to tensor before passing on as fastai Image, we can use pil2tensor\n",
    "        return vision.Image(px=pil2tensor(img, np.float32).div_(255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 256\n",
    "bs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create image data bunch\n",
    "tfms = get_transforms(do_flip=True,flip_vert=True,max_rotate=360,max_warp=0,max_zoom=1.2,max_lighting=0.5,p_lighting=0.5, p_affine=0.5)\n",
    "src = (MyImageItemList.from_df(df=df2,path='./',cols='path') #get dataset from dataset\n",
    "        .split_by_rand_pct(0.9) #Splitting the dataset\n",
    "#        .split_by_idx(val_idx)\n",
    "        .label_from_df(cols='level'\n",
    "#                        ,\n",
    "#                        label_delim=','\n",
    "#                        label_cls=FloatList\n",
    "                      ) #obtain labels from the level column\n",
    "      )\n",
    "data= (src.transform(tfms,size=sz,resize_method=ResizeMethod.SQUISH,padding_mode='zeros') #Data augmentation\n",
    "        .databunch(bs=bs,num_workers=0) #DataBunch\n",
    "        .normalize(imagenet_stats) #Normalize\n",
    "       )\n",
    "# data = ImageDataBunch.from_df('./', \n",
    "#                               df=df_train, \n",
    "#                               valid_pct=0.2,\n",
    "#                               folder=\"../input/aptos2019-blindness-detection/train_images\",\n",
    "#                               suffix=\".png\",\n",
    "#                               ds_tfms=get_transforms(flip_vert=True, max_warp=0),\n",
    "#                               size=224,\n",
    "#                               bs=64, \n",
    "#                               num_workers=0,\n",
    "#                              label_col='label', label_delim=',').normalize(imagenet_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: \n",
      " [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# check classes\n",
    "print(f'Classes: \\n {data.classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some sample images\n",
    "# data.show_batch(rows=3, figsize=(7,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageDataBunch;\n",
       "\n",
       "Train: LabelList (367 items)\n",
       "x: MyImageItemList\n",
       "Image (3, 256, 256),Image (3, 256, 256),Image (3, 256, 256),Image (3, 256, 256),Image (3, 256, 256)\n",
       "y: CategoryList\n",
       "4,2,4,2,0\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (3295 items)\n",
       "x: MyImageItemList\n",
       "Image (3, 256, 256),Image (3, 256, 256),Image (3, 256, 256),Image (3, 256, 256),Image (3, 256, 256)\n",
       "y: CategoryList\n",
       "0,2,1,0,4\n",
       "Path: .;\n",
       "\n",
       "Test: None"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(arr):\n",
    "    mask = arr = 0\n",
    "#     print(np.where(mask.any(1), mask.argmax(1), 5))\n",
    "    return np.clip(np.where(mask.any(1), mask.argmax(1), 5) - 1, 0, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(arr):\n",
    "#     mask = arr == 0\n",
    "    print('printing array...\\n')\n",
    "    print(arr)\n",
    "    print('arr type')\n",
    "    print(type(arr))\n",
    "    print('printing get_preds...')\n",
    "    try:\n",
    "        print(np.argmax(arr, axis=1))\n",
    "    except:\n",
    "        print('not possible.')\n",
    "    print('printing shape of arr')\n",
    "    try:\n",
    "        print(arr.size())\n",
    "    except:\n",
    "        try:\n",
    "            print(arr.shape)\n",
    "        except:\n",
    "            print(len(arr))\n",
    "    try:\n",
    "#         return np.argmax(arr.T, axis=1)\n",
    "        return arr\n",
    "    except:\n",
    "        return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [\n",
    "    [0.2, 0.05, 0.6, 0.04, 0.01],\n",
    "    [0.05, 0.78, 0.02, 0.12, 0.03],\n",
    "    [0.12841, -7.6266, -6.3899, -2.1333, -0.48995],\n",
    "    [0.68119, 1.7226, -1.9895, -0.097746, 0.53576]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True],\n",
       "       [False, False, False, False,  True],\n",
       "       [ True, False, False, False, False],\n",
       "       [ True,  True, False, False,  True]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_output = torch.tensor([\n",
    "    [1.7226, 1.7226, 1.7226, 1.7226, 1.7226],\n",
    "    [0, 0, 0, 0, 1.7226],\n",
    "    [0.12841, -7.6266, -6.3899, -2.1333, -0.48995],\n",
    "    [0.68119, 1.7226, -1.9895, -0.097746, 0.53576]\n",
    "])\n",
    "arr = (torch.sigmoid(last_output) > 0.5).numpy(); arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing array...\n",
      "\n",
      "tensor([[ 1.7226,  1.7226,  1.7226,  1.7226,  1.7226],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.7226],\n",
      "        [ 0.1284, -7.6266, -6.3899, -2.1333, -0.4900],\n",
      "        [ 0.6812,  1.7226, -1.9895, -0.0977,  0.5358]])\n",
      "arr type\n",
      "<class 'torch.Tensor'>\n",
      "printing get_preds...\n",
      "tensor([0, 4, 0, 1])\n",
      "printing shape of arr\n",
      "torch.Size([4, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7226,  1.7226,  1.7226,  1.7226,  1.7226],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.7226],\n",
       "        [ 0.1284, -7.6266, -6.3899, -2.1333, -0.4900],\n",
       "        [ 0.6812,  1.7226, -1.9895, -0.0977,  0.5358]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_preds(last_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing array...\n",
      "\n",
      "[[ True  True  True  True  True]\n",
      " [False False False False  True]\n",
      " [ True False False False False]\n",
      " [ True  True False False  True]]\n",
      "arr type\n",
      "<class 'numpy.ndarray'>\n",
      "printing get_preds...\n",
      "[0 4 0 0]\n",
      "printing shape of arr\n",
      "(4, 5)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-98bf15539183>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mget_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'all'"
     ]
    }
   ],
   "source": [
    "# Test output\n",
    "assert (get_preds(arr) == np.array([4, 0, 0, 1])).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler = [i for i in data.valid_dl.sampler]\n",
    "# sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfusionMatrix(Callback):\n",
    "    \"Computes the confusion matrix.\"\n",
    "\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        self.n_classes = 0\n",
    "\n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.cm = None\n",
    "\n",
    "    def on_batch_end(self, last_output:Tensor, last_target:Tensor, **kwargs):\n",
    "        preds = torch.tensor(get_preds((torch.sigmoid(last_output) > 0.5).cpu().numpy()))\n",
    "        \n",
    "        targs = torch.tensor(get_preds(last_target.cpu().numpy()))\n",
    "\n",
    "        if self.n_classes == 0:\n",
    "            self.n_classes = last_output.shape[-1]\n",
    "            self.x = torch.arange(0, self.n_classes)\n",
    "        \n",
    "        cm = ((preds==self.x[:, None]) & (targs==self.x[:, None, None])).sum(dim=2, dtype=torch.float32)\n",
    "        if self.cm is None: self.cm =  cm\n",
    "        else:               self.cm += cm\n",
    "\n",
    "    def on_epoch_end(self, **kwargs):\n",
    "        self.metric = self.cm\n",
    "        \n",
    "\n",
    "@dataclass\n",
    "class KappaScore(ConfusionMatrix):\n",
    "    \"Compute the rate of agreement (Cohens Kappa).\"\n",
    "    weights:Optional[str]=None      # None, `linear`, or `quadratic`\n",
    "\n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        sum0 = self.cm.sum(dim=0)\n",
    "        sum1 = self.cm.sum(dim=1)\n",
    "        expected = torch.einsum('i,j->ij', (sum0, sum1)) / sum0.sum()\n",
    "        if self.weights is None:\n",
    "            w = torch.ones((self.n_classes, self.n_classes))\n",
    "            w[self.x, self.x] = 0\n",
    "        elif self.weights == \"linear\" or self.weights == \"quadratic\":\n",
    "            w = torch.zeros((self.n_classes, self.n_classes))\n",
    "            w += torch.arange(self.n_classes, dtype=torch.float)\n",
    "            w = torch.abs(w - torch.t(w)) if self.weights == \"linear\" else (w - torch.t(w)) ** 2\n",
    "        else: raise ValueError('Unknown weights. Expected None, \"linear\", or \"quadratic\".')\n",
    "        k = torch.sum(w * self.cm) / torch.sum(w * expected)\n",
    "        return add_metrics(last_metrics, 1-k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "def quadratic_kappa(y_hat, y):\n",
    "    return torch.tensor(cohen_kappa_score(torch.round(y_hat), y, weights='quadratic'),device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, logits=False, reduction='elementwise_mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        else:\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction is None:\n",
    "            return F_loss\n",
    "        else:\n",
    "            return torch.mean(F_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-eaa101f4688e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'efficientnet-b0'\u001b[0m \u001b[1;31m# change b5 to b0-b4 for different versions of EfficientNet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEfficientNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mlearn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLearner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkappa\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_thresh\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'models'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m# build model (use resnet34)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# learn = create_cnn(data, models.resnet34, metrics=[kappa, accuracy_thresh], model_dir=\"/tmp/model/\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<string>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, model, opt_func, loss_func, metrics, true_wd, bn_wd, wd, train_bn, path, model_dir, callback_fns, callbacks, layer_groups, add_time, silent)\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\fastai\\basic_train.py\u001b[0m in \u001b[0;36m__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;34m\"Setup path,metrics, callbacks and ensure model directory exists.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mifnone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_func\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlistify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    197\u001b[0m                 \u001b[1;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    161\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m    162\u001b[0m     \u001b[0m_check_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[0m_cudart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudaGetErrorName\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: unknown error"
     ]
    }
   ],
   "source": [
    "kappa = KappaScore()\n",
    "kappa.weights = \"quadratic\"\n",
    "model_name = 'efficientnet-b0' # change b5 to b0-b4 for different versions of EfficientNet\n",
    "model = EfficientNet.from_pretrained(model_name, num_classes=5)\n",
    "learn = Learner(data, model, metrics = [kappa, accuracy_thresh], model_dir='models')\n",
    "# build model (use resnet34)\n",
    "# learn = create_cnn(data, models.resnet34, metrics=[kappa, accuracy_thresh], model_dir=\"/tmp/model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.loss_fn = FocalLoss()\n",
    "learn.loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find(start_lr=1e-7, num_it=300, end_lr=10)\n",
    "# learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage = 1\n",
    "csvlogger = callbacks.CSVLogger(learn=learn, filename='history_stage_'+str(stage)+'_'+model_name, append=True)\n",
    "saveModel = callbacks.SaveModelCallback(learn, every='epoch',\n",
    "                                        monitor='quadratic_kappa', mode='max',\n",
    "                                        name='stage_'+str(stage)+'bestmodel_'+model_name+'_'+str(sz)+'_mse')\n",
    "reduceLR = callbacks.ReduceLROnPlateauCallback(learn=learn, monitor = 'quadratic_kappa', mode = 'max', patience = 1, factor = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='5', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/5 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>kappa_score</th>\n",
       "      <th>accuracy_thresh</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='progress-bar-interrupted' max='103', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      Interrupted\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing array...\n",
      "\n",
      "[[False False  True False False]\n",
      " [ True False  True False False]\n",
      " [False False  True False False]\n",
      " [ True False  True False False]\n",
      " ...\n",
      " [False False  True False False]\n",
      " [ True False  True False False]\n",
      " [ True False False False False]\n",
      " [ True False  True False False]]\n",
      "arr type\n",
      "<class 'numpy.ndarray'>\n",
      "printing get_preds...\n",
      "[2 0 2 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 2 0 2 0 0 0]\n",
      "printing shape of arr\n",
      "(32, 5)\n",
      "printing array...\n",
      "\n",
      "[4 2 2 0 1 2 2 0 2 0 0 2 2 0 4 2 3 2 0 2 4 0 0 0 0 2 2 2 3 2 0 0]\n",
      "arr type\n",
      "<class 'numpy.ndarray'>\n",
      "printing get_preds...\n",
      "not possible.\n",
      "printing shape of arr\n",
      "(32,)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (5) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-b1599997ca00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# first time learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcsvlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaveModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduceLR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/train.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, final_div, wd, callbacks, tot_epochs, start_epoch)\u001b[0m\n\u001b[1;32m     20\u001b[0m     callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor, pct_start=pct_start,\n\u001b[1;32m     21\u001b[0m                                        final_div=final_div, tot_epochs=tot_epochs, start_epoch=start_epoch))\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mLearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_div\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcb_fns_registered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_validate\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 val_loss = validate(learn.model, learn.data.valid_dl, loss_func=learn.loss_func,\n\u001b[0;32m--> 106\u001b[0;31m                                        cb_handler=cb_handler, pbar=pbar)\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, dl, loss_func, cb_handler, pbar, average, n_batch)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mnums\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_el\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_batch\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mnums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/callback.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;34m\"Handle end of processing one batch with `loss`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'last_loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch_end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall_mets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iteration'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/callback.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, cb_name, call_mets, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;34m\"Call through to all of the `CallbakHandler` functions.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcall_mets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mmet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_and_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_and_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/fastai/callback.py\u001b[0m in \u001b[0;36m_call_and_update\u001b[0;34m(self, cb, cb_name, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_and_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;34m\"Call `cb_name` on `cb` and update the inner state.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mifnone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'on_{cb_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-11687c21ffab>\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, last_output, last_target, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtargs\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mcm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (5) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "# first time learning\n",
    "lr = 1e-2\n",
    "learn.fit_one_cycle(5, max_lr=lr, callbacks=[csvlogger, saveModel, reduceLR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir('../input/models/')\n",
    "except:\n",
    "    print('Directory already exists.')\n",
    "! cp -r ../input/models models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLinks\n",
    "FileLinks('.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
